# -*- coding: utf-8 -*-
"""UCMP_CCC_CC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tJ_6sdeEprQJ_P8pdFEbc8XGm1ORrltb

**Unleashing Code Migration Potential using Contexual-Code-Completion and Code Correction**
"""

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch

!pip install --upgrade sympy transformers

import random
from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
import torch
import pandas as pd

# Load CodeSearchNet dataset
dataset = load_dataset('code_search_net', 'all')

# Filter the dataset to include only Python, Java, and JavaScript instances
def filter_languages(example):
    return example['language'] in ['python', 'java', 'javascript']

filtered_dataset = dataset.filter(filter_languages)

# Function to filter out short functions
def is_long_enough(example):
    return len(example['whole_func_string'].split()) >= 10

# Modified apply_ccc function that doesn't return None
def apply_ccc(example):
    code = example['whole_func_string']
    tokens = code.split()

    min_mask_length = max(2, len(tokens) // 5)
    max_mask_length = min(len(tokens) // 2, 50)

    if min_mask_length >= max_mask_length:
        mask_length = min_mask_length
    else:
        mask_length = random.randint(min_mask_length, max_mask_length)

    mask_start = random.randint(0, len(tokens) - mask_length)

    input_tokens = tokens[:mask_start] + ['<mask>'] * mask_length + tokens[mask_start + mask_length:]
    target_tokens = tokens[mask_start:mask_start + mask_length]

    return {
        'input_code': ' '.join(input_tokens),
        'target_code': ' '.join(target_tokens),
        'language': example['language']
    }

# First, filter out short functions
filtered_dataset = filtered_dataset.filter(is_long_enough)

# Then apply CCC and remove unnecessary columns
ccc_dataset = filtered_dataset.map(
    apply_ccc,
    remove_columns=['whole_func_string', 'func_code_string', 'func_documentation_string', 'func_code_tokens', 'func_documentation_tokens', 'func_code_url']
)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Load the configuration of the pretrained model
pretrained_config = T5Config.from_pretrained('Salesforce/codet5-base')

# Load the pre-trained CodeT5 model directly using the pretrained config
model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base', config=pretrained_config)

# Tokenize the dataset
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding='max_length', truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding='max_length', truncation=True, max_length=512)
    return {
        'input_ids': inputs['input_ids'],
        'attention_mask': inputs['attention_mask'],
        'labels': targets['input_ids']
    }

tokenized_dataset = ccc_dataset.map(tokenize_function, batched=True)

# Save the tokenized dataset to disk
tokenized_dataset.save_to_disk("tokenized_dataset")

# Create a zip archive of the saved dataset
import shutil
shutil.make_archive("tokenized_dataset", "zip", "tokenized_dataset")

print("Dataset has been saved and zipped as 'tokenized_dataset.zip'")

"""**Checkpoint after CCC mapping and Tokenization**"""

import random
from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
import torch
import pandas as pd
import zipfile
import os
import shutil
from datasets import load_from_disk


# Specify the path to your zip file
zip_path = "/content/tokenized_dataset_CCC.zip"

# Specify the directory where you want to extract the contents
extract_dir = "/content/extracted_dataset"

# Create the extraction directory if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

# Extract the contents of the zip file
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# Load the dataset from the extracted directory
tokenized_dataset = load_from_disk(extract_dir)

print("Dataset has been successfully loaded from the zip file.")

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Load the configuration of the pretrained model
pretrained_config = T5Config.from_pretrained('Salesforce/codet5-base')

# Load the pre-trained CodeT5 model directly using the pretrained config
model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base', config=pretrained_config)

# Initialize Accelerator
accelerator = Accelerator()

# Assuming 'model', 'tokenizer', and 'tokenized_dataset' are already defined

# Define the number of samples you want to use for training and validation
num_train_samples = 20000
num_eval_samples = 500

# Get the number of samples in the original datasets
num_train_data = len(tokenized_dataset['train'])
num_eval_data = len(tokenized_dataset['validation'])

# Create indices for sampling
train_indices = list(range(num_train_data))
eval_indices = list(range(num_eval_data))

# Shuffle the indices
random.shuffle(train_indices)
random.shuffle(eval_indices)

# Select the first num_train_samples indices for training
train_indices = train_indices[:num_train_samples]
# Select the first num_eval_samples indices for validation
eval_indices = eval_indices[:num_eval_samples]

# Select samples based on the indices
tokenized_train_dataset = tokenized_dataset['train'].select(train_indices)
tokenized_eval_dataset = tokenized_dataset['validation'].select(eval_indices)

# Set up training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="steps",
    save_steps=100,
    eval_steps=100,
    report_to="none"
)

# Define compute_metrics function to compute accuracy
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = torch.argmax(logits, dim=-1)

    accuracy = (predictions == labels).float().mean().item()

    return {"accuracy": accuracy}

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_eval_dataset,
)

# Prepare everything with accelerator
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model,
    trainer.optimizer,
    trainer.get_train_dataloader(),
    trainer.get_eval_dataloader()
)

# Train the model
train_result = trainer.train()

# Log history to DataFrame
log_history = pd.DataFrame(trainer.state.log_history)

# Save the model after training is complete
output_dir = '/content/CCC_PretrainedModel_1epoch/'
model.save_pretrained(output_dir)

# Step 2: Zip the Folder
shutil.make_archive('/content/CCC_PretrainedModel_1epoch', 'zip', '/content/CCC_PretrainedModel_1epoch')

# Set the new number of total epochs
additional_epochs = 9
training_args.num_train_epochs = 1 + additional_epochs  # 1 already done + 9 more

# Resume training from the last checkpoint
train_result = trainer.train(resume_from_checkpoint=True)

# Save the model after training is complete
output_dir = '/content/CCC_PretrainedModel_10epoch/'
model.save_pretrained(output_dir)

# Step 2: Zip the Folder
shutil.make_archive('/content/CCC_PretrainedModel_10epoch', 'zip', '/content/CCC_PretrainedModel_10epoch')

"""**Checkpoint after Pre-training the model using Contextual Code Mapping**

# Few-Shot Learning and BLEU-4 Calculation
"""

fewshot_sample_size = []
bleu_score_PJ_UCMP = []
bleu_score_CJ_UCMP = []

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch

from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
from matplotlib import pyplot as plt
import seaborn as sns
import torch
import random
import collections
import math
import pandas as pd
import zipfile
import os
from datasets import load_from_disk

# Define the path to the ZIP file
zip_file_path = '/content/fewshotdata.zip'  # Replace with your ZIP file path

# Define the directory to extract to
extract_dir = '/content/FewShotData'  # Replace with your desired extraction directory

# Check if the extraction directory exists, if not, create it
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the contents of the extraction directory to verify
extracted_files = os.listdir(extract_dir)
print("Files extracted successfully:", extracted_files)

# Step 2: Navigate to the zip file location and define the destination path
zip_file_path = '/content/CCC_PretrainedModel_10epoch.zip'  # Replace with the actual path to your zip file
destination_path = './saved_model'  # Replace with the actual path to your destination folder

# Step 3: Unzip the file
import zipfile
import os

# Create destination directory if it does not exist
if not os.path.exists(destination_path):
    os.makedirs(destination_path)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_path)

print(f'Unzipped files saved to {destination_path}')

# Code for BLEU
import math
import collections

def _get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
    return ngram_counts

def compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    precisions = [0] * max_order
    for i in range(0, max_order):
        if smooth:
            precisions[i] = (matches_by_order[i] + 1.) / (possible_matches_by_order[i] + 1.)
        else:
            if possible_matches_by_order[i] > 0:
                precisions[i] = float(matches_by_order[i]) / possible_matches_by_order[i]
            else:
                precisions[i] = 0.0

    if min(precisions) > 0:
        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)
        geo_mean = math.exp(p_log_sum)
    else:
        geo_mean = 0

    ratio = float(translation_length) / reference_length

    if ratio > 1.0:
        bp = 1.
    else:
        bp = math.exp(1 - 1. / ratio)

    bleu = geo_mean * bp

    return bleu, precisions, bp, ratio, translation_length, reference_length

def compute_metrics(pred):
    labels = pred.label_ids
    if isinstance(pred.predictions, tuple):
        predictions = pred.predictions[0]
    else:
        predictions = pred.predictions

    preds = predictions.argmax(-1)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Tokenize the decoded predictions and labels
    tokenized_preds = [pred.split() for pred in decoded_preds]
    tokenized_labels = [[label.split()] for label in decoded_labels]

    # Compute BLEU score using custom function
    bleu_score, _, _, _, _, _ = compute_bleu(tokenized_labels, tokenized_preds)

    # Print decoded predictions for the first 5 examples
    print("Decoded Predictions:")
    for i in range(min(5, len(decoded_preds))):
        pred = decoded_preds[i]
        label = decoded_labels[i]
        print(f"Example {i + 1}:")
        print(f"Prediction: {pred}")
        print(f"Reference: {label}")
        print()

    return {'bleu': bleu_score}

!pip install evaluate

from datasets import Dataset, DatasetDict
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
from evaluate import load  # Instead of from datasets import load_metric
#from datasets import load_metric
from accelerate import Accelerator
import pandas as pd
import matplotlib.pyplot as plt

fewshot_sample_size.append(8)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_bleu']
csharp_java_bleu = csharp_java_eval_results['eval_bleu']


bleu_score_PJ_UCMP.append(python_java_bleu)
bleu_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java BLEU:", python_java_bleu)
print("C# to Java BLEU:", csharp_java_bleu)

print(fewshot_sample_size)
print(bleu_score_PJ_UCMP)
print(bleu_score_CJ_UCMP)

"""# Few-shot Learning on 16 samples"""

fewshot_sample_size.append(16)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_16',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_bleu']
csharp_java_bleu = csharp_java_eval_results['eval_bleu']


bleu_score_PJ_UCMP.append(python_java_bleu)
bleu_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java BLEU:", python_java_bleu)
print("C# to Java BLEU:", csharp_java_bleu)

print(fewshot_sample_size)
print(bleu_score_PJ_UCMP)
print(bleu_score_CJ_UCMP)

"""# Few-shot Learning on 32 samples"""

fewshot_sample_size.append(32)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_32',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_bleu']
csharp_java_bleu = csharp_java_eval_results['eval_bleu']


bleu_score_PJ_UCMP.append(python_java_bleu)
bleu_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java BLEU:", python_java_bleu)
print("C# to Java BLEU:", csharp_java_bleu)

print(fewshot_sample_size)
print(bleu_score_PJ_UCMP)
print(bleu_score_CJ_UCMP)

"""# Few-shot Learning on 100 samples"""

fewshot_sample_size.append(100)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_100',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_bleu']
csharp_java_bleu = csharp_java_eval_results['eval_bleu']


bleu_score_PJ_UCMP.append(python_java_bleu)
bleu_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java BLEU:", python_java_bleu)
print("C# to Java BLEU:", csharp_java_bleu)

print(fewshot_sample_size)
print(bleu_score_PJ_UCMP)
print(bleu_score_CJ_UCMP)

"""# Few-shot Learning on 500 samples"""

fewshot_sample_size.append(500)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_500',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_bleu']
csharp_java_bleu = csharp_java_eval_results['eval_bleu']


bleu_score_PJ_UCMP.append(python_java_bleu)
bleu_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java BLEU:", python_java_bleu)
print("C# to Java BLEU:", csharp_java_bleu)

print(fewshot_sample_size)
print(bleu_score_PJ_UCMP)
print(bleu_score_CJ_UCMP)

"""# Few-shot Learning on 1000 samples"""

fewshot_sample_size.append(1000)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_1000',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_csharp_java.train()
trainer_python_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_bleu']
csharp_java_bleu = csharp_java_eval_results['eval_bleu']

bleu_score_PJ_UCMP.append(python_java_bleu)
bleu_score_CJ_UCMP.append(csharp_java_bleu)

# Print evaluation results
print("Python to Java BLEU:", python_java_bleu)
print("C# to Java BLEU:", csharp_java_bleu)

print(fewshot_sample_size)
print(bleu_score_PJ_UCMP)
print(bleu_score_CJ_UCMP)

"""# Few-Shot Learning With EM Calculation

# Few-shot Learning on 8 samples - Exact Match
"""

fewshot_sample_size = []
em_score_PJ_UCMP = []
em_score_CJ_UCMP = []

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch

from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
from matplotlib import pyplot as plt
import seaborn as sns
import torch
import random
import collections
import math
import pandas as pd
import zipfile
import os
from datasets import load_from_disk

# Define the path to the ZIP file
zip_file_path = '/content/fewshotdata.zip'  # Replace with your ZIP file path

# Define the directory to extract to
extract_dir = '/content/FewShotData'  # Replace with your desired extraction directory

# Check if the extraction directory exists, if not, create it
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the contents of the extraction directory to verify
extracted_files = os.listdir(extract_dir)
print("Files extracted successfully:", extracted_files)

# Step 2: Navigate to the zip file location and define the destination path
zip_file_path = '/content/CCC_PretrainedModel_10epoch.zip'  # Replace with the actual path to your zip file
destination_path = './saved_model'  # Replace with the actual path to your destination folder

# Step 3: Unzip the file
import zipfile
import os

# Create destination directory if it does not exist
if not os.path.exists(destination_path):
    os.makedirs(destination_path)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_path)

print(f'Unzipped files saved to {destination_path}')

def compute_exact_match(decoded_preds, decoded_labels):
    """
    Calculate the Exact Match (EM) score.

    :param decoded_preds: A list of decoded predictions.
    :param decoded_labels: A list of decoded labels.
    :return: The Exact Match score as a percentage.
    """
    correct = sum(pred == label for pred, label in zip(decoded_preds, decoded_labels))
    total = len(decoded_preds)
    em_score = (correct / total) * 100
    return em_score

def compute_metrics(pred):
    labels = pred.label_ids
    if isinstance(pred.predictions, tuple):
        predictions = pred.predictions[0]
    else:
        predictions = pred.predictions

    preds = predictions.argmax(-1)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Compute Exact Match score
    exact_match_score = compute_exact_match(decoded_preds, decoded_labels)

    # Print decoded predictions for the first 5 examples
    print("Decoded Predictions:")
    for i in range(min(5, len(decoded_preds))):
        pred = decoded_preds[i]
        label = decoded_labels[i]
        print(f"Example {i + 1}:")
        print(f"Prediction: {pred}")
        print(f"Reference: {label}")
        print()

    return {'exact_match': exact_match_score}

!pip install evaluate
from evaluate import load  # Instead of from datasets import load_metric
from datasets import Dataset, DatasetDict
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as pd
import matplotlib.pyplot as plt

fewshot_sample_size.append(8)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_exact_match']
csharp_java_bleu = csharp_java_eval_results['eval_exact_match']


em_score_PJ_UCMP.append(python_java_bleu)
em_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java EM:", python_java_bleu)
print("C# to Java EM:", csharp_java_bleu)

print(fewshot_sample_size)
print(em_score_PJ_UCMP)
print(em_score_CJ_UCMP)

"""# Few-shot Learning on 16 samples"""

fewshot_sample_size.append(16)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_16',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_exact_match']
csharp_java_bleu = csharp_java_eval_results['eval_exact_match']


em_score_PJ_UCMP.append(python_java_bleu)
em_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java EM:", python_java_bleu)
print("C# to Java EM:", csharp_java_bleu)

print(fewshot_sample_size)
print(em_score_PJ_UCMP)
print(em_score_CJ_UCMP)

"""# Few-shot Learning on 32 samples"""

fewshot_sample_size.append(32)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_32',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_exact_match']
csharp_java_bleu = csharp_java_eval_results['eval_exact_match']


em_score_PJ_UCMP.append(python_java_bleu)
em_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java EM:", python_java_bleu)
print("C# to Java EM:", csharp_java_bleu)

print(fewshot_sample_size)
print(em_score_PJ_UCMP)
print(em_score_CJ_UCMP)

"""# Few-shot Learning on 100 samples"""

fewshot_sample_size.append(100)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_100',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_exact_match']
csharp_java_bleu = csharp_java_eval_results['eval_exact_match']


em_score_PJ_UCMP.append(python_java_bleu)
em_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java EM:", python_java_bleu)
print("C# to Java EM:", csharp_java_bleu)

print(fewshot_sample_size)
print(em_score_PJ_UCMP)
print(em_score_CJ_UCMP)

"""# Few-shot Learning on 500 samples"""

fewshot_sample_size.append(500)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_500',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_exact_match']
csharp_java_bleu = csharp_java_eval_results['eval_exact_match']


em_score_PJ_UCMP.append(python_java_bleu)
em_score_CJ_UCMP.append(csharp_java_bleu)


# Print evaluation results
print("Python to Java EM:", python_java_bleu)
print("C# to Java EM:", csharp_java_bleu)

print(fewshot_sample_size)
print(em_score_PJ_UCMP)
print(em_score_CJ_UCMP)

"""# Few-shot Learning on 1000 samples"""

fewshot_sample_size.append(1000)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.java"

# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}



# Directory where you saved your model
output_dir = './saved_model'


# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)


# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)
    return {'input_ids': inputs.input_ids, 'attention_mask': inputs.attention_mask, 'labels': targets.input_ids}

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_1000',
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=10,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_csharp_java.train()
trainer_python_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store BLEU and CodeBLEU scores
python_java_bleu = python_java_eval_results['eval_exact_match']
csharp_java_bleu = csharp_java_eval_results['eval_exact_match']

em_score_PJ_UCMP.append(python_java_bleu)
em_score_CJ_UCMP.append(csharp_java_bleu)

# Print evaluation results
print("Python to Java EM:", python_java_bleu)
print("C# to Java EM:", csharp_java_bleu)

print(fewshot_sample_size)
print(em_score_PJ_UCMP)
print(em_score_CJ_UCMP)

"""# Few-Shot Learning CodeBLEU Calculation

# Alternate CodeBLEU execution
"""

# !pip list | grep tree-sitter-java  # Check if tree-sitter-java is installed
# !pip uninstall -y tree-sitter-java  # Uninstall the current version
# !pip install tree-sitter-java==0.23.2

fewshot_sample_size = []
codebleu_score_PJ_UCMP = []
codebleu_score_CJ_UCMP = []

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch
!pip install evaluate

!pip install codebleu
!pip install tree-sitter-java==0.21.0

from datasets import load_dataset, DatasetDict, Dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
from matplotlib import pyplot as plt
import seaborn as sns
import torch
import random
import collections
import math
import pandas as pd
import zipfile
import os
from datasets import load_from_disk
from evaluate import load  # Instead of from datasets import load_metric

# Define the path to the ZIP file
zip_file_path = '/content/fewshotdata.zip'  # Replace with your ZIP file path

# Define the directory to extract to
extract_dir = '/content/FewShotData'  # Replace with your desired extraction directory

# Check if the extraction directory exists, if not, create it
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the contents of the extraction directory to verify
extracted_files = os.listdir(extract_dir)
print("Files extracted successfully:", extracted_files)

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Navigate to the zip file location and define the destination path
zip_file_path = '/content/drive/My Drive/Mitotic_Classification/MITOS-14_base.zip'  # Replace with the actual path to your zip file
destination_path = './saved_model'  # Replace with the actual path to your destination folder

# Step 3: Unzip the file
import zipfile
import os

# Create destination directory if it does not exist
if not os.path.exists(destination_path):
    os.makedirs(destination_path)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_path)

print(f'Unzipped files saved to {destination_path}')

import os
from transformers import RobertaTokenizer  # Ensure you have imported RobertaTokenizer
from codebleu import calc_codebleu

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Define the function to compute CodeBLEU
def compute_codebleu(preds, refs):
    codebleu_score = calc_codebleu(refs, preds, lang='java', weights=(0.25,0.25,0.25,0.25), tokenizer=tokenizer)
    return codebleu_score

# Custom metric function to calculate CodeBLEU
def compute_metrics(pred):
    labels = pred.label_ids
    if isinstance(pred.predictions, tuple):
        predictions = pred.predictions[0]
    else:
        predictions = pred.predictions

    preds = predictions.argmax(-1)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Tokenize the decoded predictions and labels
    tokenized_preds = [pred.split() for pred in decoded_preds]
    tokenized_labels = [[label.split()] for label in decoded_labels]

    # Calculate CodeBLEU
    codebleu_score = compute_codebleu(decoded_preds, decoded_labels)

    return {'codebleu': codebleu_score}

    from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Append few-shot sample size
fewshot_sample_size.append(8)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']

# Append CodeBLEU scores to the lists
codebleu_score_PJ_UCMP.append(python_java_codebleu['codebleu'])
codebleu_score_CJ_UCMP.append(csharp_java_codebleu['codebleu'])

# Print evaluation results
print("Python to Java CodeBLEU:", python_java_codebleu)
print("C# to Java CodeBLEU:", csharp_java_codebleu)

print(fewshot_sample_size)
print(codebleu_score_PJ_UCMP)
print(codebleu_score_CJ_UCMP)

"""# Few-shot Learning on 16 samples - CodeBLEU"""

# Append few-shot sample size
fewshot_sample_size.append(16)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_16',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']

# Append CodeBLEU scores to the lists
codebleu_score_PJ_UCMP.append(python_java_codebleu['codebleu'])
codebleu_score_CJ_UCMP.append(csharp_java_codebleu['codebleu'])

# Print evaluation results
print("Python to Java CodeBLEU:", python_java_codebleu)
print("C# to Java CodeBLEU:", csharp_java_codebleu)

print(fewshot_sample_size)
print(codebleu_score_PJ_UCMP)
print(codebleu_score_CJ_UCMP)

"""# Few-shot Learning on 32 samples - CodeBLEU"""

# Append few-shot sample size
fewshot_sample_size.append(32)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_32',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']

# Append CodeBLEU scores to the lists
codebleu_score_PJ_UCMP.append(python_java_codebleu['codebleu'])
codebleu_score_CJ_UCMP.append(csharp_java_codebleu['codebleu'])

# Print evaluation results
print("Python to Java CodeBLEU:", python_java_codebleu)
print("C# to Java CodeBLEU:", csharp_java_codebleu)

print(fewshot_sample_size)
print(codebleu_score_PJ_UCMP)
print(codebleu_score_CJ_UCMP)

"""# Few-shot Learning on 100 samples - CodeBLEU"""

# Append few-shot sample size
fewshot_sample_size.append(100)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_100',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']

# Append CodeBLEU scores to the lists
codebleu_score_PJ_UCMP.append(python_java_codebleu['codebleu'])
codebleu_score_CJ_UCMP.append(csharp_java_codebleu['codebleu'])

# Print evaluation results
print("Python to Java CodeBLEU:", python_java_codebleu)
print("C# to Java CodeBLEU:", csharp_java_codebleu)

print(fewshot_sample_size)
print(codebleu_score_PJ_UCMP)
print(codebleu_score_CJ_UCMP)

"""# Few-shot Learning on 500 samples - CodeBLEU"""

# Append few-shot sample size
fewshot_sample_size.append(500)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_500',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']

# Append CodeBLEU scores to the lists
codebleu_score_PJ_UCMP.append(python_java_codebleu['codebleu'])
codebleu_score_CJ_UCMP.append(csharp_java_codebleu['codebleu'])

# Print evaluation results
print("Python to Java CodeBLEU:", python_java_codebleu)
print("C# to Java CodeBLEU:", csharp_java_codebleu)

print(fewshot_sample_size)
print(codebleu_score_PJ_UCMP)
print(codebleu_score_CJ_UCMP)

"""# Few-shot Learning on 1000 samples - CodeBLEU"""

# Append few-shot sample size
fewshot_sample_size.append(1000)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_1000',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']

# Append CodeBLEU scores to the lists
codebleu_score_PJ_UCMP.append(python_java_codebleu['codebleu'])
codebleu_score_CJ_UCMP.append(csharp_java_codebleu['codebleu'])

# Print evaluation results
print("Python to Java CodeBLEU:", python_java_codebleu)
print("C# to Java CodeBLEU:", csharp_java_codebleu)

print(fewshot_sample_size)
print(codebleu_score_PJ_UCMP)
print(codebleu_score_CJ_UCMP)

"""# Implementnig all 3 metrics at the same time for Novel Contextual Code Completion pre-training [Final Evaluation using same hyperparameters for both CCC and CodeT5]"""

# Initialize empty lists for Python to Java and C# to Java evaluations

# For Python to Java (PJ)
codebleu_score_PJ_CCC = []
bleu4_score_PJ_CCC = []
exact_match_score_PJ_CCC = []

# For C# to Java (CJ)
codebleu_score_CJ_CCC = []
bleu4_score_CJ_CCC = []
exact_match_score_CJ_CCC = []

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch
!pip install evaluate

from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
from matplotlib import pyplot as plt
import seaborn as sns
import torch
import random
import collections
import math
import pandas as pd
import zipfile
import os
from datasets import load_from_disk
from evaluate import load

!pip install codebleu
!pip install tree-sitter-java==0.21.0

# Define the path to the ZIP file
zip_file_path = '/content/fewshotdata.zip'  # Replace with your ZIP file path

# Define the directory to extract to
extract_dir = '/content/FewShotData'  # Replace with your desired extraction directory

# Check if the extraction directory exists, if not, create it
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the contents of the extraction directory to verify
extracted_files = os.listdir(extract_dir)
print("Files extracted successfully:", extracted_files)

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Navigate to the zip file location and define the destination path
zip_file_path = '/content/drive/My Drive/FinalModels/CCC_PretrainedModel_10epoch.zip'  # Replace with the actual path to your zip file
destination_path = './saved_model'  # Replace with the actual path to your destination folder

# Step 3: Unzip the file
import zipfile
import os

# Create destination directory if it does not exist
if not os.path.exists(destination_path):
    os.makedirs(destination_path)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_path)

print(f'Unzipped files saved to {destination_path}')

from transformers import RobertaTokenizer
from codebleu import calc_codebleu
import collections
import math

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Function to compute CodeBLEU
def compute_codebleu(preds, refs):
    codebleu_score = calc_codebleu(refs, preds, lang='java', weights=(0.25,0.25,0.25,0.25), tokenizer=tokenizer)
    return codebleu_score

# Function to calculate BLEU
def _get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
    return ngram_counts

def compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    precisions = [0] * max_order
    for i in range(0, max_order):
        if smooth:
            precisions[i] = (matches_by_order[i] + 1.) / (possible_matches_by_order[i] + 1.)
        else:
            if possible_matches_by_order[i] > 0:
                precisions[i] = matches_by_order[i] / possible_matches_by_order[i]
            else:
                precisions[i] = 0.0

    if min(precisions) > 0:
        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)
        geo_mean = math.exp(p_log_sum)
    else:
        geo_mean = 0

    ratio = translation_length / reference_length
    bp = math.exp(1 - 1. / ratio) if ratio < 1.0 else 1.0
    bleu = geo_mean * bp
    return bleu

# Function to calculate Exact Match (EM)
def compute_exact_match(decoded_preds, decoded_labels):
    em_count = sum([1 for pred, label in zip(decoded_preds, decoded_labels) if pred == label])
    em_score = em_count / len(decoded_preds) if len(decoded_preds) > 0 else 0
    return em_score

def _get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
    return ngram_counts

def compute_ngram_match_score(reference_corpus, translation_corpus, max_order=4):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    total_matches = 0
    total_possible_matches = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        # Compute n-gram counts for the translation
        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)

        translation_ngram_counts = _get_ngrams(translation, max_order)

        # Calculate overlap
        overlap = translation_ngram_counts & merged_ref_ngram_counts

        # Count n-gram matches by order
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]

        # Count possible n-grams by order
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    # Calculate total matches and possible matches
    for i in range(max_order):
        total_matches += matches_by_order[i]
        total_possible_matches += possible_matches_by_order[i]

    # Return the total n-gram match score (normalized by total possible matches)
    ngram_match_score = total_matches / total_possible_matches if total_possible_matches > 0 else 0.0

    return ngram_match_score

def compute_weighted_ngram_match_score(reference_corpus, translation_corpus, max_order=4, weights=None):
    if weights is None:
        weights = [1] * max_order  # Default: equal weights for all n-grams

    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    weighted_precision = 0
    for i in range(0, max_order):
        precision = matches_by_order[i] / possible_matches_by_order[i] if possible_matches_by_order[i] > 0 else 0.0
        weighted_precision += weights[i] * precision

    return weighted_precision

# Custom metric function to calculate CodeBLEU, BLEU-4, and EM
def compute_metrics(pred):
    labels = pred.label_ids
    if isinstance(pred.predictions, tuple):
        predictions = pred.predictions[0]
    else:
        predictions = pred.predictions

    preds = predictions.argmax(-1)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Tokenize for BLEU calculation
    tokenized_preds = [pred.split() for pred in decoded_preds]
    tokenized_labels = [[label.split()] for label in decoded_labels]

    # Calculate BLEU-4
    bleu_score = compute_bleu(tokenized_labels, tokenized_preds, max_order=4)

    # Calculate Exact Match (EM)
    em_score = compute_exact_match(decoded_preds, decoded_labels)

    # Calculate CodeBLEU
    codebleu_score = compute_codebleu(decoded_preds, decoded_labels)

    # Calculate n-gram match score
    ngram_match_score = compute_ngram_match_score(tokenized_labels, tokenized_preds, max_order=4)

    weights = [0.1, 0.2, 0.3, 0.4]
    # Calculate Weighted N-gram Match Score
    weighted_ngram_match_score = compute_weighted_ngram_match_score(tokenized_labels, tokenized_preds, max_order=4, weights=weights)

    CodeBLEU = 0.25*ngram_match_score + 0.25*weighted_ngram_match_score + 0.25*codebleu_score['syntax_match_score'] + 0.25*codebleu_score['dataflow_match_score']

    return {
        'bleu-4': bleu_score,
        'exact_match': em_score,
        'codebleu': CodeBLEU,
        'ngram_match_score': ngram_match_score,
        'weighted_ngram_match_score': weighted_ngram_match_score,
        'syntax_match_score': codebleu_score['syntax_match_score'],
        'dataflow_match_score': codebleu_score['dataflow_match_score']
    }

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

"""# Comparing CCC Pre-trained model with CodeT5 base model"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Initialize empty lists for Python to Java and C# to Java evaluations

# For Python to Java (PJ)
codebleu_score_PJ_CCC = []
bleu4_score_PJ_CCC = []
exact_match_score_PJ_CCC = []

# For C# to Java (CJ)
codebleu_score_CJ_CCC = []
bleu4_score_CJ_CCC = []
exact_match_score_CJ_CCC = []

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch
!pip install evaluate

from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
from matplotlib import pyplot as plt
import seaborn as sns
import torch
import random
import collections
import math
import pandas as pd
import zipfile
import os
from datasets import load_from_disk
from evaluate import load

!pip install codebleu
!pip install tree-sitter-java==0.21.0

# Define the path to the ZIP file
zip_file_path = '/content/fewshotdata.zip'  # Replace with your ZIP file path

# Define the directory to extract to
extract_dir = '/content/FewShotData'  # Replace with your desired extraction directory

# Check if the extraction directory exists, if not, create it
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the contents of the extraction directory to verify
extracted_files = os.listdir(extract_dir)
print("Files extracted successfully:", extracted_files)

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Load the configuration of the pretrained model
pretrained_config = T5Config.from_pretrained('Salesforce/codet5-base')

# Load the pre-trained CodeT5 model directly using the pretrained config
model = T5ForConditionalGeneration.from_pretrained('Salesforce/codet5-base', config=pretrained_config)

# Define the path where you want to save the model and tokenizer
save_directory = './saved_model'

# Save the model and tokenizer to the specified directory
model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

print(f'Model and tokenizer saved to {save_directory}')

from transformers import RobertaTokenizer
from codebleu import calc_codebleu
import collections
import math

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Function to compute CodeBLEU
def compute_codebleu(preds, refs):
    codebleu_score = calc_codebleu(refs, preds, lang='java', weights=(0.25,0.25,0.25,0.25), tokenizer=tokenizer)
    return codebleu_score

# Function to calculate BLEU
def _get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
    return ngram_counts

def compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    precisions = [0] * max_order
    for i in range(0, max_order):
        if smooth:
            precisions[i] = (matches_by_order[i] + 1.) / (possible_matches_by_order[i] + 1.)
        else:
            if possible_matches_by_order[i] > 0:
                precisions[i] = matches_by_order[i] / possible_matches_by_order[i]
            else:
                precisions[i] = 0.0

    if min(precisions) > 0:
        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)
        geo_mean = math.exp(p_log_sum)
    else:
        geo_mean = 0

    ratio = translation_length / reference_length
    bp = math.exp(1 - 1. / ratio) if ratio < 1.0 else 1.0
    bleu = geo_mean * bp
    return bleu

# Function to calculate Exact Match (EM)
def compute_exact_match(decoded_preds, decoded_labels):
    em_count = sum([1 for pred, label in zip(decoded_preds, decoded_labels) if pred == label])
    em_score = em_count / len(decoded_preds) if len(decoded_preds) > 0 else 0
    return em_score

def _get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
    return ngram_counts

def compute_ngram_match_score(reference_corpus, translation_corpus, max_order=4):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    total_matches = 0
    total_possible_matches = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        # Compute n-gram counts for the translation
        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)

        translation_ngram_counts = _get_ngrams(translation, max_order)

        # Calculate overlap
        overlap = translation_ngram_counts & merged_ref_ngram_counts

        # Count n-gram matches by order
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]

        # Count possible n-grams by order
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    # Calculate total matches and possible matches
    for i in range(max_order):
        total_matches += matches_by_order[i]
        total_possible_matches += possible_matches_by_order[i]

    # Return the total n-gram match score (normalized by total possible matches)
    ngram_match_score = total_matches / total_possible_matches if total_possible_matches > 0 else 0.0

    return ngram_match_score

def compute_weighted_ngram_match_score(reference_corpus, translation_corpus, max_order=4, weights=None):
    if weights is None:
        weights = [1] * max_order  # Default: equal weights for all n-grams

    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    weighted_precision = 0
    for i in range(0, max_order):
        precision = matches_by_order[i] / possible_matches_by_order[i] if possible_matches_by_order[i] > 0 else 0.0
        weighted_precision += weights[i] * precision

    return weighted_precision

# Custom metric function to calculate CodeBLEU, BLEU-4, and EM
def compute_metrics(pred):
    labels = pred.label_ids
    if isinstance(pred.predictions, tuple):
        predictions = pred.predictions[0]
    else:
        predictions = pred.predictions

    preds = predictions.argmax(-1)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Tokenize for BLEU calculation
    tokenized_preds = [pred.split() for pred in decoded_preds]
    tokenized_labels = [[label.split()] for label in decoded_labels]

    # Calculate BLEU-4
    bleu_score = compute_bleu(tokenized_labels, tokenized_preds, max_order=4)

    # Calculate Exact Match (EM)
    em_score = compute_exact_match(decoded_preds, decoded_labels)

    # Calculate CodeBLEU
    codebleu_score = compute_codebleu(decoded_preds, decoded_labels)

    # Calculate n-gram match score
    ngram_match_score = compute_ngram_match_score(tokenized_labels, tokenized_preds, max_order=4)

    weights = [0.1, 0.2, 0.3, 0.4]
    # Calculate Weighted N-gram Match Score
    weighted_ngram_match_score = compute_weighted_ngram_match_score(tokenized_labels, tokenized_preds, max_order=4, weights=weights)

    CodeBLEU = 0.25*ngram_match_score + 0.25*weighted_ngram_match_score + 0.25*codebleu_score['syntax_match_score'] + 0.25*codebleu_score['dataflow_match_score']

    return {
        'bleu-4': bleu_score,
        'exact_match': em_score,
        'codebleu': CodeBLEU,
        'ngram_match_score': ngram_match_score,
        'weighted_ngram_match_score': weighted_ngram_match_score,
        'syntax_match_score': codebleu_score['syntax_match_score'],
        'dataflow_match_score': codebleu_score['dataflow_match_score']
    }

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_8/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_16/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_32/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_100/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_500/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

# Experiment conducted on same training parameters on CodeT5-base
# Novel pre-training called as Contextual Code Mapping is applied

import matplotlib.pyplot as plt

# Define fewshot_sample_size, BLEU, EM, and CodeBLEU scores for CodeT5-base
fewshot_sample_size = [8, 16, 32, 100, 500, 1000]
bleu_score_PJ_UCMP = [0.550283, 0.70085, 0.760193, 0.844674, 0.871241, 0.884193]
bleu_score_CJ_UCMP = [0.522311, 0.480906, 0.408083, 0.656708, 0.712286, 0.791608]
# Multiply each element by 100
bleu_score_PJ_UCMP = [score * 100 for score in bleu_score_PJ_UCMP]
bleu_score_CJ_UCMP = [score * 100 for score in bleu_score_CJ_UCMP]

em_score_PJ_UCMP = [0, 0, 6.25, 7, 10, 17]
em_score_CJ_UCMP = [0, 25, 34.37, 44, 50, 66]
# Fill here (Epochs = 20 [for las one 5], Batch size = 4)
codebleu_score_PJ_UCMP = [0.5195, 0.622401, 0.654364, 0.754607, 0.784427, 0.808563]
codebleu_score_CJ_UCMP = [0.556665, 0.574394, 0.564266, 0.701207, 0.747347, 0.814425]
# Multiply each element by 100
codebleu_score_PJ_UCMP = [score * 100 for score in codebleu_score_PJ_UCMP]
codebleu_score_CJ_UCMP = [score * 100 for score in codebleu_score_CJ_UCMP]

# Define BLEU, EM, and CodeBLEU scores for CCC pretraining
bleu_score_PJ_CCC = [0.6990, 0.7559, 0.7927, 0.8525, 0.8737, 0.8903]
bleu_score_CJ_CCC = [0.5888, 0.4206, 0.4475, 0.6580, 0.7245, 0.8102]
# Multiply each element by 100
bleu_score_PJ_CCC = [score * 100 for score in bleu_score_PJ_CCC]
bleu_score_CJ_CCC = [score * 100 for score in bleu_score_CJ_CCC]

em_score_PJ_CCC = [0, 0, 3.125, 7, 9, 19]
em_score_CJ_CCC = [12.5, 31.25, 31.25, 44, 52, 66]
codebleu_score_PJ_CCC = [0.624158, 0.672313, 0.710192, 0.767833, 0.787858, 0.814552]
codebleu_score_CJ_CCC = [0.644957, 0.559511, 0.585894, 0.677952, 0.748007, 0.827913]
# Multiply each element by 100
codebleu_score_PJ_CCC = [score * 100 for score in codebleu_score_PJ_CCC]
codebleu_score_CJ_CCC = [score * 100 for score in codebleu_score_CJ_CCC]

# Create subplots
fig, axs = plt.subplots(3, 2, figsize=(15, 15))

# First graph: Python to Java BLEU
axs[0, 0].plot(fewshot_sample_size, bleu_score_PJ_UCMP, marker='s', linestyle='-', color='b', label='CodeT5')
axs[0, 0].plot(fewshot_sample_size, bleu_score_PJ_CCC, marker='o', linestyle='--', color='cyan', label='Contextual Code Completion')
axs[0, 0].set_title('Python to Java BLEU-4')
axs[0, 0].set_xlabel('Fewshot Sample Size')
axs[0, 0].set_ylabel('BLEU-4 Score')
axs[0, 0].set_xscale('log')
axs[0, 0].set_xticks(fewshot_sample_size)
axs[0, 0].set_xticklabels(fewshot_sample_size)
axs[0, 0].grid(True, which="both", ls="--")
axs[0, 0].legend()

# Second graph: C# to Java BLEU
axs[0, 1].plot(fewshot_sample_size, bleu_score_CJ_UCMP, marker='s', linestyle='-', color='r', label='CodeT5')
axs[0, 1].plot(fewshot_sample_size, bleu_score_CJ_CCC, marker='o', linestyle='--', color='orange', label='Contextual Code Completion')
axs[0, 1].set_title('C# to Java BLEU-4')
axs[0, 1].set_xlabel('Fewshot Sample Size')
axs[0, 1].set_ylabel('BLEU-4 Score')
axs[0, 1].set_xscale('log')
axs[0, 1].set_xticks(fewshot_sample_size)
axs[0, 1].set_xticklabels(fewshot_sample_size)
axs[0, 1].grid(True, which="both", ls="--")
axs[0, 1].legend()

# Third graph: Python to Java Exact Match
axs[1, 0].plot(fewshot_sample_size, em_score_PJ_UCMP, marker='s', linestyle='-', color='g', label='CodeT5')
axs[1, 0].plot(fewshot_sample_size, em_score_PJ_CCC, marker='o', linestyle='--', color='lime', label='Contextual Code Completion')
axs[1, 0].set_title('Python to Java Exact Match')
axs[1, 0].set_xlabel('Fewshot Sample Size')
axs[1, 0].set_ylabel('EM Score')
axs[1, 0].set_xscale('log')
axs[1, 0].set_xticks(fewshot_sample_size)
axs[1, 0].set_xticklabels(fewshot_sample_size)
axs[1, 0].grid(True, which="both", ls="--")
axs[1, 0].legend()

# Fourth graph: C# to Java Exact Match
axs[1, 1].plot(fewshot_sample_size, em_score_CJ_UCMP, marker='s', linestyle='-', color='m', label='CodeT5')
axs[1, 1].plot(fewshot_sample_size, em_score_CJ_CCC, marker='o', linestyle='--', color='purple', label='Contextual Code Completion')
axs[1, 1].set_title('C# to Java Exact Match')
axs[1, 1].set_xlabel('Fewshot Sample Size')
axs[1, 1].set_ylabel('EM Score')
axs[1, 1].set_xscale('log')
axs[1, 1].set_xticks(fewshot_sample_size)
axs[1, 1].set_xticklabels(fewshot_sample_size)
axs[1, 1].grid(True, which="both", ls="--")
axs[1, 1].legend()

# Fifth graph: Python to Java CodeBLEU
axs[2, 0].plot(fewshot_sample_size, codebleu_score_PJ_UCMP, marker='s', linestyle='-', color='c', label='CodeT5')
axs[2, 0].plot(fewshot_sample_size, codebleu_score_PJ_CCC, marker='o', linestyle='--', color='blue', label='Contextual Code Completion')
axs[2, 0].set_title('Python to Java CodeBLEU')
axs[2, 0].set_xlabel('Fewshot Sample Size')
axs[2, 0].set_ylabel('CodeBLEU Score')
axs[2, 0].set_xscale('log')
axs[2, 0].set_xticks(fewshot_sample_size)
axs[2, 0].set_xticklabels(fewshot_sample_size)
axs[2, 0].grid(True, which="both", ls="--")
axs[2, 0].legend()

# Sixth graph: C# to Java CodeBLEU
axs[2, 1].plot(fewshot_sample_size, codebleu_score_CJ_UCMP, marker='s', linestyle='-', color='y', label='CodeT5')
axs[2, 1].plot(fewshot_sample_size, codebleu_score_CJ_CCC, marker='o', linestyle='--', color='gold', label='Contextual Code Completion')
axs[2, 1].set_title('C# to Java CodeBLEU')
axs[2, 1].set_xlabel('Fewshot Sample Size')
axs[2, 1].set_ylabel('CodeBLEU Score')
axs[2, 1].set_xscale('log')
axs[2, 1].set_xticks(fewshot_sample_size)
axs[2, 1].set_xticklabels(fewshot_sample_size)
axs[2, 1].grid(True, which="both", ls="--")
axs[2, 1].legend()

# Adjust layout and show plot
plt.tight_layout()
plt.show()

# Set up the plots
bar_width = 0.35
index = np.arange(len(FewShotLearning_Sample_Size))

fig, axes = plt.subplots(1, 2, figsize=(14, 7))

# Python to Java Match Score
axes[0].bar(index - bar_width/2,
            SyntaxMatchScore_CodeT5_python_to_Java,
            bar_width,
            label='CodeT5 Syntax Match',
            color=(0.2, 0.4, 0.8, 0.5)) # Transparent blue
axes[0].bar(index + bar_width/2,
            SyntaxMatchScore_CCC_python_to_Java,
            bar_width,
            label='CCC Syntax Match',
            color=(0.8, 0.4, 0.2, 0.5)) # Transparent orange
axes[0].step(index - bar_width/2,
             DataFlowMatchScore_CodeT5_python_to_Java,
             label='CodeT5 DataFlow Match',
             where='mid',
             linestyle='dashed',
             color=(0.2, 0.8, 0.4, 0.7)) # Transparent green
axes[0].step(index + bar_width/2,
             DataFlowMatchScore_CCC_python_to_Java,
             label='CCC DataFlow Match',
             where='mid',
             linestyle='dashed',
             color=(0.8, 0.2, 0.6, 0.7)) # Transparent purple

axes[0].set_xlabel('Few-Shot Training Sample Sizes')
axes[0].set_ylabel('Match Score')
axes[0].set_title('Python to Java Translation')
axes[0].set_xticks(index)
axes[0].set_xticklabels(FewShotLearning_Sample_Size)
axes[0].set_ylim(30, 100)  # Set y-axis range
axes[0].legend()

# C# to Java Match Score
axes[1].bar(index - bar_width/2,
            SyntaxMatchScore_CodeT5_CSharp_to_Java,
            bar_width,
            label='CodeT5 Syntax Match',
            color=(0.2, 0.4, 0.8, 0.5)) # Transparent blue
axes[1].bar(index + bar_width/2,
            SyntaxMatchScore_CCC_CSharp_to_Java,
            bar_width,
            label='CCC Syntax Match',
            color=(0.8, 0.4, 0.2, 0.5)) # Transparent orange
axes[1].step(index - bar_width/2,
             DataFlowMatchScore_CodeT5_CSharp_to_Java,
             label='CodeT5 DataFlow Match',
             where='mid',
             linestyle='dashed',
             color=(0.2, 0.8, 0.4, 0.7)) # Transparent green
axes[1].step(index + bar_width/2,
             DataFlowMatchScore_CCC_CSharp_to_Java,
             label='CCC DataFlow Match',
             where='mid',
             linestyle='dashed',
             color=(0.8, 0.2, 0.6, 0.7)) # Transparent purple

axes[1].set_xlabel('Few-Shot Training Sample Sizes')
axes[1].set_ylabel('Match Score')
axes[1].set_title('C# to Java Translation')
axes[1].set_xticks(index)
axes[1].set_xticklabels(FewShotLearning_Sample_Size)
axes[1].set_ylim(30, 100)  # Set y-axis range
axes[1].legend()

plt.tight_layout()
plt.show()

"""# Saving the CCC Few-Shot Learning model and finding the structural equivalance and semantic correctness"""

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Initialize empty lists for Python to Java and C# to Java evaluations

# For Python to Java (PJ)
codebleu_score_PJ_CCC = []
bleu4_score_PJ_CCC = []
exact_match_score_PJ_CCC = []

# For C# to Java (CJ)
codebleu_score_CJ_CCC = []
bleu4_score_CJ_CCC = []
exact_match_score_CJ_CCC = []

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch
!pip install evaluate

from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
from matplotlib import pyplot as plt
import seaborn as sns
import torch
import random
import collections
import math
import pandas as pd
import zipfile
import os
from datasets import load_from_disk
from evaluate import load

!pip install codebleu
!pip install tree-sitter-java==0.21.0

# Define the path to the ZIP file
zip_file_path = '/content/fewshotdata.zip'  # Replace with your ZIP file path

# Define the directory to extract to
extract_dir = '/content/FewShotData'  # Replace with your desired extraction directory

# Check if the extraction directory exists, if not, create it
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

# List the contents of the extraction directory to verify
extracted_files = os.listdir(extract_dir)
print("Files extracted successfully:", extracted_files)

# Step 2: Navigate to the zip file location and define the destination path
zip_file_path = '/content/drive/My Drive/FinalModels/CCC_PretrainedModel_10epoch.zip'  # Replace with the actual path to your zip file
destination_path = './saved_model'  # Replace with the actual path to your destination folder

# Step 3: Unzip the file
import zipfile
import os

# Create destination directory if it does not exist
if not os.path.exists(destination_path):
    os.makedirs(destination_path)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_path)

print(f'Unzipped files saved to {destination_path}')

from transformers import RobertaTokenizer
from codebleu import calc_codebleu
import collections
import math

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Function to compute CodeBLEU
def compute_codebleu(preds, refs):
    codebleu_score = calc_codebleu(refs, preds, lang='java', weights=(0.25,0.25,0.25,0.25), tokenizer=tokenizer)
    return codebleu_score

# Function to calculate BLEU
def _get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
    return ngram_counts

def compute_bleu(reference_corpus, translation_corpus, max_order=4, smooth=False):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    precisions = [0] * max_order
    for i in range(0, max_order):
        if smooth:
            precisions[i] = (matches_by_order[i] + 1.) / (possible_matches_by_order[i] + 1.)
        else:
            if possible_matches_by_order[i] > 0:
                precisions[i] = matches_by_order[i] / possible_matches_by_order[i]
            else:
                precisions[i] = 0.0

    if min(precisions) > 0:
        p_log_sum = sum((1. / max_order) * math.log(p) for p in precisions)
        geo_mean = math.exp(p_log_sum)
    else:
        geo_mean = 0

    ratio = translation_length / reference_length
    bp = math.exp(1 - 1. / ratio) if ratio < 1.0 else 1.0
    bleu = geo_mean * bp
    return bleu

# Function to calculate Exact Match (EM)
def compute_exact_match(decoded_preds, decoded_labels):
    em_count = sum([1 for pred, label in zip(decoded_preds, decoded_labels) if pred == label])
    em_score = em_count / len(decoded_preds) if len(decoded_preds) > 0 else 0
    return em_score

def _get_ngrams(segment, max_order):
    ngram_counts = collections.Counter()
    for order in range(1, max_order + 1):
        for i in range(len(segment) - order + 1):
            ngram = tuple(segment[i:i + order])
            ngram_counts[ngram] += 1
    return ngram_counts

def compute_ngram_match_score(reference_corpus, translation_corpus, max_order=4):
    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    total_matches = 0
    total_possible_matches = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        # Compute n-gram counts for the translation
        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)

        translation_ngram_counts = _get_ngrams(translation, max_order)

        # Calculate overlap
        overlap = translation_ngram_counts & merged_ref_ngram_counts

        # Count n-gram matches by order
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]

        # Count possible n-grams by order
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    # Calculate total matches and possible matches
    for i in range(max_order):
        total_matches += matches_by_order[i]
        total_possible_matches += possible_matches_by_order[i]

    # Return the total n-gram match score (normalized by total possible matches)
    ngram_match_score = total_matches / total_possible_matches if total_possible_matches > 0 else 0.0

    return ngram_match_score

def compute_weighted_ngram_match_score(reference_corpus, translation_corpus, max_order=4, weights=None):
    if weights is None:
        weights = [1] * max_order  # Default: equal weights for all n-grams

    matches_by_order = [0] * max_order
    possible_matches_by_order = [0] * max_order
    reference_length = 0
    translation_length = 0

    for references, translation in zip(reference_corpus, translation_corpus):
        reference_length += min(len(r) for r in references)
        translation_length += len(translation)

        merged_ref_ngram_counts = collections.Counter()
        for reference in references:
            merged_ref_ngram_counts |= _get_ngrams(reference, max_order)
        translation_ngram_counts = _get_ngrams(translation, max_order)
        overlap = translation_ngram_counts & merged_ref_ngram_counts
        for ngram in overlap:
            matches_by_order[len(ngram) - 1] += overlap[ngram]
        for order in range(1, max_order + 1):
            possible_matches = len(translation) - order + 1
            if possible_matches > 0:
                possible_matches_by_order[order - 1] += possible_matches

    weighted_precision = 0
    for i in range(0, max_order):
        precision = matches_by_order[i] / possible_matches_by_order[i] if possible_matches_by_order[i] > 0 else 0.0
        weighted_precision += weights[i] * precision

    return weighted_precision

# Custom metric function to calculate CodeBLEU, BLEU-4, and EM
def compute_metrics(pred):
    labels = pred.label_ids
    if isinstance(pred.predictions, tuple):
        predictions = pred.predictions[0]
    else:
        predictions = pred.predictions

    preds = predictions.argmax(-1)
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Tokenize for BLEU calculation
    tokenized_preds = [pred.split() for pred in decoded_preds]
    tokenized_labels = [[label.split()] for label in decoded_labels]

    # Calculate BLEU-4
    bleu_score = compute_bleu(tokenized_labels, tokenized_preds, max_order=4)

    # Calculate Exact Match (EM)
    em_score = compute_exact_match(decoded_preds, decoded_labels)

    # Calculate CodeBLEU
    codebleu_score = compute_codebleu(decoded_preds, decoded_labels)

    # Calculate n-gram match score
    ngram_match_score = compute_ngram_match_score(tokenized_labels, tokenized_preds, max_order=4)

    weights = [0.1, 0.2, 0.3, 0.4]
    # Calculate Weighted N-gram Match Score
    weighted_ngram_match_score = compute_weighted_ngram_match_score(tokenized_labels, tokenized_preds, max_order=4, weights=weights)

    CodeBLEU = 0.25*ngram_match_score + 0.25*weighted_ngram_match_score + 0.25*codebleu_score['syntax_match_score'] + 0.25*codebleu_score['dataflow_match_score']

    return {
        'bleu-4': bleu_score,
        'exact_match': em_score,
        'codebleu': CodeBLEU,
        'ngram_match_score': ngram_match_score,
        'weighted_ngram_match_score': weighted_ngram_match_score,
        'syntax_match_score': codebleu_score['syntax_match_score'],
        'dataflow_match_score': codebleu_score['dataflow_match_score']
    }

from datasets import Dataset, DatasetDict  # Ensure DatasetDict is imported
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from transformers import DataCollatorForSeq2Seq
from transformers import pipeline
#from datasets import load_metric
from accelerate import Accelerator
import pandas as plt
import matplotlib.pyplot as plt

# Initialize the tokenizer for CodeT5
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')  # Use the correct tokenizer

# Function to load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Directory where you saved your model
output_dir = './saved_model'

# Load the model
model = T5ForConditionalGeneration.from_pretrained(output_dir)

# Tokenization function
def tokenize_function(example):
    inputs = tokenizer(example['input_code'], padding="max_length", truncation=True, max_length=512)
    targets = tokenizer(example['target_code'], padding="max_length", truncation=True, max_length=512)

    # Ensure that the labels are correctly formatted
    inputs['labels'] = targets['input_ids']

    # Flatten the lists to avoid nested lists issue
    inputs['labels'] = [label if label != tokenizer.pad_token_id else -100 for label in inputs['labels']]

    return inputs

# Create tokenized datasets
tokenized_python_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in python_java_data.items()
})

tokenized_csharp_java_datasets = DatasetDict({
    split: Dataset.from_dict(data).map(tokenize_function, batched=True)
    for split, data in csharp_java_data.items()
})

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/fewShot_8',
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    evaluation_strategy='epoch',
    logging_dir='./logs',
    logging_steps=10,
    save_steps=1000,
    num_train_epochs=20,
    report_to="none"
)

# Initialize Trainer for Python to Java
trainer_python_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_python_java_datasets['train'],
    eval_dataset=tokenized_python_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Initialize Trainer for C# to Java
trainer_csharp_java = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_csharp_java_datasets['train'],
    eval_dataset=tokenized_csharp_java_datasets['valid'],
    compute_metrics=compute_metrics,
)

# Train the models
trainer_python_java.train()
trainer_csharp_java.train()

# Evaluate the models
python_java_eval_results = trainer_python_java.evaluate()
csharp_java_eval_results = trainer_csharp_java.evaluate()

# Store CodeBLEU, BLEU-4, and Exact Match scores
python_java_codebleu = python_java_eval_results['eval_codebleu']
python_java_bleu4 = python_java_eval_results['eval_bleu-4']
python_java_exact_match = python_java_eval_results['eval_exact_match']

csharp_java_codebleu = csharp_java_eval_results['eval_codebleu']
csharp_java_bleu4 = csharp_java_eval_results['eval_bleu-4']
csharp_java_exact_match = csharp_java_eval_results['eval_exact_match']

# Append CodeBLEU, BLEU-4, and Exact Match scores to respective lists
codebleu_score_PJ_CCC.append(python_java_codebleu)
bleu4_score_PJ_CCC.append(python_java_bleu4)
exact_match_score_PJ_CCC.append(python_java_exact_match)

codebleu_score_CJ_CCC.append(csharp_java_codebleu)
bleu4_score_CJ_CCC.append(csharp_java_bleu4)
exact_match_score_CJ_CCC.append(csharp_java_exact_match)

# Print evaluation results
print("Python to Java - CodeBLEU:", python_java_codebleu)
print("Python to Java - BLEU-4:", python_java_bleu4)
print("Python to Java - Exact Match:", python_java_exact_match)

print("C# to Java - CodeBLEU:", csharp_java_codebleu)
print("C# to Java - BLEU-4:", csharp_java_bleu4)
print("C# to Java - Exact Match:", csharp_java_exact_match)

# Save Python to Java model
trainer_python_java.save_model('./saved_model/CCC_python_java')

# Save C# to Java model
trainer_csharp_java.save_model('./saved_model/CCC_csharp_java')

"""## AST Similarity and Compilability"""

!pip install --upgrade transformers accelerate
!pip install datasets tokenizers torch
!pip install evaluate

from datasets import load_dataset
from transformers import RobertaTokenizer, T5Config, T5ForConditionalGeneration, Trainer, TrainingArguments
from accelerate import Accelerator
from matplotlib import pyplot as plt
import seaborn as sns
import torch
import random
import collections
import math
import pandas as pd
import zipfile
import os
from datasets import load_from_disk
from evaluate import load

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 3: Unzip the file
import zipfile
import os

# Step 2: Navigate to the zip file location and define the destination path
zip_file_path = '/content/drive/My Drive/FinalModels/CCC_python_Java.zip'  # Replace with the actual path to your zip file
destination_path = './saved_model/CCC_python_Java'  # Replace with the actual path to your destination folder

# Create destination directory if it does not exist
if not os.path.exists(destination_path):
    os.makedirs(destination_path)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_path)

print(f'Unzipped files saved to {destination_path}')

# Step 2: Navigate to the zip file location and define the destination path
zip_file_path = '/content/drive/My Drive/FinalModels/CCC_csharp_Java.zip'  # Replace with the actual path to your zip file
destination_path = './saved_model/CCC_csharp_Java'  # Replace with the actual path to your destination folder

# Create destination directory if it does not exist
if not os.path.exists(destination_path):
    os.makedirs(destination_path)

# Unzip the file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(destination_path)

print(f'Unzipped files saved to {destination_path}')

# Unzip few-shot data
zip_file_path = '/content/drive/My Drive/fewshotdata.zip'  # Replace with the actual path to your zip file
extract_dir = '/content/FewShotData'  # Replace with your desired extraction directory
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)
print("Files extracted successfully:", os.listdir(extract_dir))



# Load data from files
def load_data(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        data = file.readlines()
    return data

# Define file paths for datasets
python_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.train"
python_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.valid"
python_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanpython.data.test"
java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.train"
java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.valid"
java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/cleanjava.data.test"

csharp_java_train_file = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.cs"
csharp_java_valid_file = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.cs"
csharp_java_test_file = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.cs"
java_train_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/train.java-cs.txt.java"
java_valid_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/valid.java-cs.txt.java"
java_test_file_csharp = "/content/FewShotData/fewshotdata/data_sample_1000/test.java-cs.txt.java"

# Load data for Python to Java
python_java_train = load_data(python_java_train_file)
python_java_valid = load_data(python_java_valid_file)
python_java_test = load_data(python_java_test_file)
java_train = load_data(java_train_file)
java_valid = load_data(java_valid_file)
java_test = load_data(java_test_file)

# Load data for C# to Java
csharp_java_train = load_data(csharp_java_train_file)
csharp_java_valid = load_data(csharp_java_valid_file)
csharp_java_test = load_data(csharp_java_test_file)
java_train_csharp = load_data(java_train_file_csharp)
java_valid_csharp = load_data(java_valid_file_csharp)
java_test_csharp = load_data(java_test_file_csharp)

# Prepare datasets for Python to Java
python_java_data = {
    'train': {'input_code': python_java_train, 'target_code': java_train},
    'valid': {'input_code': python_java_valid, 'target_code': java_valid},
    'test': {'input_code': python_java_test, 'target_code': java_test},
}

# Prepare datasets for C# to Java
csharp_java_data = {
    'train': {'input_code': csharp_java_train, 'target_code': java_train_csharp},
    'valid': {'input_code': csharp_java_valid, 'target_code': java_valid_csharp},
    'test': {'input_code': csharp_java_test, 'target_code': java_test_csharp},
}

# Load the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Load the Python-to-Java model and tokenizer
python_java_model_path = './saved_model/CCC_python_Java'
python_java_model = T5ForConditionalGeneration.from_pretrained(python_java_model_path).to(device)
python_java_tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Load the C#-to-Java model and tokenizer
csharp_java_model_path = './saved_model/CCC_csharp_Java'
csharp_java_model = T5ForConditionalGeneration.from_pretrained(csharp_java_model_path).to(device)
csharp_java_tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

# Translation function
def translate_code(input_codes, model, tokenizer):
    translated_codes = []
    for code in input_codes:
        input_ids = tokenizer.encode(code, return_tensors="pt", max_length=512, truncation=True).to(device)
        outputs = model.generate(input_ids, max_length=512)
        translated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)
        translated_codes.append(translated_code)
    return translated_codes

# Translate the test data
python_java_translations = translate_code(python_java_data['test']['input_code'], python_java_model, python_java_tokenizer)
csharp_java_translations = translate_code(csharp_java_data['test']['input_code'], csharp_java_model, csharp_java_tokenizer)

# Save the translations to new files
output_dir = '/content/Translations'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

with open(os.path.join(output_dir, 'PythonJavaTranslation.txt'), 'w', encoding='utf-8') as f:
    for translation in python_java_translations:
        f.write(translation + '\n')

with open(os.path.join(output_dir, 'CSharpJavaTranslation.txt'), 'w', encoding='utf-8') as f:
    for translation in csharp_java_translations:
        f.write(translation + '\n')

# Verify the contents of the output directory
print("Translation files created successfully:", os.listdir(output_dir))

import shutil


# Step 2: Zip the Folder
shutil.make_archive('/content/Translations', 'zip', '/content/Translations')

import re

def snake_to_camel(snake_str):
    components = snake_str.split('_')
    # Capitalize the first letter of each component except the first one
    camel_str = components[0] + ''.join(x.title() for x in components[1:])
    return camel_str

def process_java_file(input_file_path, output_file_path):
    # Open files with newline='' to prevent double newline issues
    with open(input_file_path, 'r', newline='') as input_file, open(output_file_path, 'w', newline='') as output_file:
        for line in input_file:
            # Remove trailing newline character from the line
            line = line.rstrip()
            # Extract function definition and name with multiple underscores
            match = re.search(r'\b(\w+(_\w+)*)\s*\(', line)
            if match:
                snake_case_name = match.group(1)
                # Convert the function name from snake_case to camelCase
                camel_case_name = snake_to_camel(snake_case_name)
                # Replace the snake_case name with camelCase name
                modified_line = line.replace(snake_case_name, camel_case_name)
                output_file.write(modified_line)
            else:
                # If no function name is found, write the line as is
                output_file.write(line + '\n')

def print_file_contents(file_path):
    with open(file_path, 'r') as file:
        contents = file.read()
        print(contents)

# Example usage:
input_file_path = '/content/Translations/PythonJavaTranslation.txt'
output_file_path = '/content/Processed_PythonJava_Translations.txt'
process_java_file(input_file_path, output_file_path)
print("Contents of the modified file:")
print_file_contents(output_file_path)

import re

def snake_to_camel(snake_str):
    components = snake_str.split('_')
    # Capitalize the first letter of each component except the first one
    camel_str = components[0] + ''.join(x.title() for x in components[1:])
    return camel_str

def process_java_file(input_file_path, output_file_path):
    # Open files with newline='' to prevent double newline issues
    with open(input_file_path, 'r', newline='') as input_file, open(output_file_path, 'w', newline='') as output_file:
        for line in input_file:
            # Remove trailing newline character from the line
            line = line.rstrip()
            # Extract function definition and name with multiple underscores
            match = re.search(r'\b(\w+(_\w+)*)\s*\(', line)
            if match:
                snake_case_name = match.group(1)
                # Convert the function name from snake_case to camelCase
                camel_case_name = snake_to_camel(snake_case_name)
                # Replace the snake_case name with camelCase name
                modified_line = line.replace(snake_case_name, camel_case_name)
                output_file.write(modified_line)
            else:
                # If no function name is found, write the line as is
                output_file.write(line + '\n')

def print_file_contents(file_path):
    with open(file_path, 'r') as file:
        contents = file.read()
        print(contents)


# Example usage:
input_file_path = '/content/Translations/CSharpJavaTranslation.txt'
output_file_path = '/content/Processed_CSharpJava_Translations.txt'
process_java_file(input_file_path, output_file_path)
print("Contents of the modified file:")
print_file_contents(output_file_path)

"""Here we will be computing 5 different values

1.   Tree Edit Distance = Minimum nuber of operations required to translate one tree into the other
2.   exact_matches = Count how many distances are 0
3.   non_exact_matches = Counts number of distances that are not 0, or simply we can say it is the number of translated code that requires change
4.   avg_distance = It gives an overall measure of how similar the translated functions are to the reference funcitons. This can be taken as a matrics for model evaluation.
5.   Percentage of exact match = How often model produces perfect translation


**Interpretation of Average Edit Distance**



*   0        -> Excellent
*   1-2      -> Very Good
*   3-5      -> Good
*   6-10     -> Moderate
*   11-20    -> Poor
*   Grt 20   -> Very Poor

Generalized



*   0-5 : Good, model translations are very close to the reference
*   6-10 : Moderate, model translations are not perfect, but are reasonably close and maybe acceptable depending on the use case
*   Greater than 10 : Bad, model translations are significantly different and requires substantial changes





**How Code Complexity can affect Average Edit Distance**



*   Simple Code : Since the structural difference is less it will have low value
*   Complex programs : Since the codes can contain many elements and complex logic, higher average edit distance can be tolerated

Python to Java Translation comparison
"""

!pip install javalang apted

import javalang
import subprocess
from apted import APTED, Config

# Unzip few-shot data
zip_file_path = '/content/ComprehensiveEvaluationDataset.zip'  # Replace with the actual path to your zip file
extract_dir = '/content/'  # Replace with your desired extraction directory
if not os.path.exists(extract_dir):
    os.makedirs(extract_dir)
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)
print("Files extracted successfully:", os.listdir(extract_dir))

# Function to parse Java code and get its AST
def get_ast(java_code):
    try:
        # Wrap the function in a class definition with a main function in one line
        wrapped_code = f"import java.io.*; import java.util.*; public class TempClass {{ public static void main(String[] args) {{  }} {java_code} }}"
        tree = javalang.parse.parse(wrapped_code)
        print("AST Generated Successfully:")
        print(tree)  # Print the complete AST for debugging
        return tree
    except javalang.parser.JavaSyntaxError as e:
        print(f"Syntax Error parsing code: {e}")
        return None
    except Exception as e:
        print(f"Error parsing code: {e}")
        import traceback
        traceback.print_exc()  # Print full traceback for debugging
        return None

# Function to check if the code compiles
def check_compilation(java_code):
    try:
        # Write the code to a temporary file
        with open('TempClass.java', 'w') as file:
            file.write(java_code)

        # Compile the Java file using `javac`
        result = subprocess.run(['javac', 'TempClass.java'], capture_output=True, text=True)

        # Return True if compilation is successful, False otherwise
        return result.returncode == 0
    except Exception as e:
        print(f"Error during compilation: {e}")
        return False

# Load translated functions
with open('/content/ComprehensiveEvaluationDataset/Processed_PythonJavaTranslation.txt', 'r') as file:
    translated_functions = file.readlines()

# Load reference functions
with open('/content/ComprehensiveEvaluationDataset/PythonJavaReference.test', 'r') as file:
    reference_functions = file.readlines()

# Ensure both files have the same number of lines
assert len(translated_functions) == len(reference_functions), "Files must have the same number of lines"

# Parse functions and generate ASTs
translated_asts = [get_ast(func) for func in translated_functions]
reference_asts = [get_ast(func) for func in reference_functions]

# Count compilable and non-compilable code snippets
compilable_translations = sum(1 for func in translated_functions if check_compilation(f"import java.io.*; import java.util.*; public class TempClass {{ public static void main(String[] args) {{  }} {func} }}"))
non_compilable_translations = len(translated_functions) - compilable_translations

# Compare ASTs
class ASTNodeConfig(Config):
    def rename(self, node1, node2):
        return 1 if node1 != node2 else 0

    def insert(self, node):
        return 1

    def delete(self, node):
        return 1

def javalang_to_apted(ast):
    def node_to_tuple(node):
        if isinstance(node, javalang.ast.Node):
            return (node.__class__.__name__, [node_to_tuple(child) for child in node.children if child])
        else:
            return (str(node), [])
    return node_to_tuple(ast)

def compare_asts(ast1, ast2):
    ast1_tuple = javalang_to_apted(ast1)
    ast2_tuple = javalang_to_apted(ast2)
    apted = APTED(ast1_tuple, ast2_tuple, ASTNodeConfig())
    return apted.compute_edit_distance()

# Compare ASTs for compilable translations
distances = [compare_asts(translated_ast, reference_ast)
             for translated_ast, reference_ast in zip(translated_asts, reference_asts)
             if translated_ast is not None and reference_ast is not None]

# Calculate and print results
total_asts = len(distances)
exact_matches = sum(1 for distance in distances if distance == 0)
non_exact_matches = total_asts - exact_matches
average_distance = sum(distances) / total_asts if total_asts > 0 else 0

print(f"Total number of ASTs compared: {total_asts}")
print(f"Number of exact matches (distance 0): {exact_matches}")
print(f"Number of non-exact matches: {non_exact_matches}")
print(f"Average edit distance: {average_distance:.2f}")

# Calculate and print percentage of exact matches
if total_asts > 0:
    exact_match_percentage = (exact_matches / total_asts) * 100
    print(f"Percentage of exact matches: {exact_match_percentage:.2f}%")

# Calculate and print percentages for AST generation and compilation
total_code_snippets = len(translated_functions)
parsed_successfully = sum(1 for ast in translated_asts if ast is not None)
parsed_failure = total_code_snippets - parsed_successfully

if total_code_snippets > 0:
    parsed_success_percentage = (parsed_successfully / total_code_snippets) * 100
    parsed_failure_percentage = (parsed_failure / total_code_snippets) * 100
    print(f"Percentage of successfully parsed translations (AST generation): {parsed_success_percentage:.2f}%")
    print(f"Percentage of failed parses (AST generation): {parsed_failure_percentage:.2f}%")

    compilation_success_percentage = (compilable_translations / total_code_snippets) * 100
    compilation_failure_percentage = (non_compilable_translations / total_code_snippets) * 100
    print(f"Percentage of successfully compiled translations: {compilation_success_percentage:.2f}%")
    print(f"Percentage of failed compilations: {compilation_failure_percentage:.2f}%")

!pip install javalang apted

import javalang
import subprocess
from apted import APTED, Config

# Function to parse Java code and get its AST
def get_ast(java_code):
    try:
        # Wrap the function in a class definition with a main function in one line
        wrapped_code = f"import java.io.*; import java.util.*; public class TempClass {{ public static void main(String[] args) {{  }} {java_code} }}"
        tree = javalang.parse.parse(wrapped_code)
        print("AST Generated Successfully:")
        print(tree)  # Print the complete AST for debugging
        return tree
    except javalang.parser.JavaSyntaxError as e:
        print(f"Syntax Error parsing code: {e}")
        return None
    except Exception as e:
        print(f"Error parsing code: {e}")
        import traceback
        traceback.print_exc()  # Print full traceback for debugging
        return None

# Function to check if the code compiles
def check_compilation(java_code):
    try:
        # Write the code to a temporary file
        with open('TempClass.java', 'w') as file:
            file.write(java_code)

        # Compile the Java file using `javac`
        result = subprocess.run(['javac', 'TempClass.java'], capture_output=True, text=True)

        # Return True if compilation is successful, False otherwise
        return result.returncode == 0
    except Exception as e:
        print(f"Error during compilation: {e}")
        return False

# Load translated functions
with open('/content/ComprehensiveEvaluationDataset/Processed_CSharpJava_Translations.txt', 'r') as file:
    translated_functions = file.readlines()

# Load reference functions
with open('/content/ComprehensiveEvaluationDataset/CSharpJavaReference.java', 'r') as file:
    reference_functions = file.readlines()

# Ensure both files have the same number of lines
assert len(translated_functions) == len(reference_functions), "Files must have the same number of lines"

# Parse functions and generate ASTs
translated_asts = [get_ast(func) for func in translated_functions]
reference_asts = [get_ast(func) for func in reference_functions]

# Count compilable and non-compilable code snippets
compilable_translations = sum(1 for func in translated_functions if check_compilation(f"import java.io.*; import java.util.*; public class TempClass {{ public static void main(String[] args) {{  }} {func} }}"))
non_compilable_translations = len(translated_functions) - compilable_translations

# Compare ASTs
class ASTNodeConfig(Config):
    def rename(self, node1, node2):
        return 1 if node1 != node2 else 0

    def insert(self, node):
        return 1

    def delete(self, node):
        return 1

def javalang_to_apted(ast):
    def node_to_tuple(node):
        if isinstance(node, javalang.ast.Node):
            return (node.__class__.__name__, [node_to_tuple(child) for child in node.children if child])
        else:
            return (str(node), [])
    return node_to_tuple(ast)

def compare_asts(ast1, ast2):
    ast1_tuple = javalang_to_apted(ast1)
    ast2_tuple = javalang_to_apted(ast2)
    apted = APTED(ast1_tuple, ast2_tuple, ASTNodeConfig())
    return apted.compute_edit_distance()

# Compare ASTs for compilable translations
distances = [compare_asts(translated_ast, reference_ast)
             for translated_ast, reference_ast in zip(translated_asts, reference_asts)
             if translated_ast is not None and reference_ast is not None]

# Calculate and print results
total_asts = len(distances)
exact_matches = sum(1 for distance in distances if distance == 0)
non_exact_matches = total_asts - exact_matches
average_distance = sum(distances) / total_asts if total_asts > 0 else 0

print(f"Total number of ASTs compared: {total_asts}")
print(f"Number of exact matches (distance 0): {exact_matches}")
print(f"Number of non-exact matches: {non_exact_matches}")
print(f"Average edit distance: {average_distance:.2f}")

# Calculate and print percentage of exact matches
if total_asts > 0:
    exact_match_percentage = (exact_matches / total_asts) * 100
    print(f"Percentage of exact matches: {exact_match_percentage:.2f}%")

# Calculate and print percentages for AST generation and compilation
total_code_snippets = len(translated_functions)
parsed_successfully = sum(1 for ast in translated_asts if ast is not None)
parsed_failure = total_code_snippets - parsed_successfully

if total_code_snippets > 0:
    parsed_success_percentage = (parsed_successfully / total_code_snippets) * 100
    parsed_failure_percentage = (parsed_failure / total_code_snippets) * 100
    print(f"Percentage of successfully parsed translations (AST generation): {parsed_success_percentage:.2f}%")
    print(f"Percentage of failed parses (AST generation): {parsed_failure_percentage:.2f}%")

    compilation_success_percentage = (compilable_translations / total_code_snippets) * 100
    compilation_failure_percentage = (non_compilable_translations / total_code_snippets) * 100
    print(f"Percentage of successfully compiled translations: {compilation_success_percentage:.2f}%")
    print(f"Percentage of failed compilations: {compilation_failure_percentage:.2f}%")